<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>寒软项目总结</title>
      <link href="/2024/03/05/hr-summary/"/>
      <url>/2024/03/05/hr-summary/</url>
      
        <content type="html"><![CDATA[<p>今天中午进行了浙江工业大学的寒假软件大赛的验收答辩，整个过程给人感觉还是比较草率的，但至少在构建的过程中还是学到了不少东西。本次比赛分成三个题目，前两个是奖励综测分，最后一个是奖励米，但最终我们选择了奖励综测分的德育导师双向选择系统。</p><h3 id="下面是我们早c晚a小组完成的成果"><a href="#下面是我们早c晚a小组完成的成果" class="headerlink" title="下面是我们早c晚a小组完成的成果"></a>下面是我们早c晚a小组完成的成果</h3><p><a href="https://github.com/Penryn/SelectionSystem-Back">后端仓库地址</a><br><a href="https://github.com/xixiIBN5100/Mutual_Mentor_Selection_front">前端仓库地址</a><br><a href="https://phlin.love/">项目服务地址</a></p><p>在本项目中我和浅浅＆勿念两个人负责该项目后端，他主要负责学生端和教师端的接口完成，我这边也主要写了用户端、管理端和系统的一些自动操作。</p><h3 id="项目的一些亮点（难点）"><a href="#项目的一些亮点（难点）" class="headerlink" title="项目的一些亮点（难点）"></a>项目的一些亮点（难点）</h3><ul><li><p>上传的头像图片有将其格式转换成jpg形式，以减小其图片内存，避免之前试用期上传头像上传半天的尴尬，当时改了我半天码，都一直没成功</p></li><li><p>将xls文件导入进数据库中，这个其实也不是特别难，主要是学校给的数据（姓名、部门、办公室、电话、邮件地址）不够齐全，导致有两个老师连邮件地址都没有，导致系统读取到这个数据时就以为没有五列数据，然后就超出范围报错。</p></li><li><p>这个系统要实现第一轮选择时间截止要让那些未被选择的学生分配教师，同时教师的学生个数最多6人，且第一轮选择时间是有会被再次更改的，一开始想用go自带的库来解决，发现不太行，然后发现有cron的定时库可以使用，但我一开始的想法是每次重新调节第一轮时间的时候来重新设定定时任务，结果没成功，后面改成了每次系统主进程异步每五分钟定时查询数据库的第一轮时间有没有发生改变，如果发送变化再重新设定时间，但感觉这样会不够准时，但这也是没有办法的办法了。后面青鸟学长说可以试试timing的库，然后也有个人好像和我说可以试试信息队列(？忘了，不知道是不是这个了)。</p><h3 id="项目可以改进的地方"><a href="#项目可以改进的地方" class="headerlink" title="项目可以改进的地方"></a>项目可以改进的地方</h3></li><li><p>首先项目有个聊天室，我这里是采用轮询的形式进行较为实时的聊天对话，但事实上可以去尝试一下websocket的方式，这是真·实时对话的正确选择，这学期一定得去实现出来。</p></li><li><p>在这次答辩中，我想给那个老师展示一下我们数据库是有对敏感数据进行加密的，结果那个老师指出我的数据库对所有用户的加密方式是相同无差别，而且他们默认数据因为是一样，所以加密的密文也相同，就是可以通过某个人的信息，从而知道其他人的信息（但是怎么说呢，你得先知道某个人的信息，然后又知道我数据库里其他用户密文和他的密文相同，基本这种情况的出现就是我数据库被黑了），当然，处理这样的方法也有，就是还有个加密方法————加盐加密（bcrypt），我记得我试用期好像是这个方法，这样就可以实现相同文字形成不同的密文。</p></li><li><p>那个老师还提到了每个用户初次登录要去强制设置一个让用户修改密码。(虽然感觉必要性不大，一般都会去改一下密码)</p></li></ul><h3 id="感悟和收获"><a href="#感悟和收获" class="headerlink" title="感悟和收获"></a>感悟和收获</h3><ul><li>因为一开始的没写数据加密，导致写完加密的方法上云后出现报错，导致我两三个小时都在debug，没研究出为什么，主要也是都是不知道为什么在阿里云那里看到数据库是全空的，反正清空了数据库就成了，让ximo含泪收下20.</li><li>还有跨域问题，之前试用期和hv是相同方法配置的，结果在这里却失败了，最后东试西试，在ximo的建议下，重新设置了cors的配置就成功了，但在这之前还搜到了nginx可以设置跨域，但在基本了解nginx后，发现运维还是实现不了跨域处理的，要处理还是得在后端方面。</li><li>也小小当了一把产品（资本家），稍微提了点可以完善的bug。<br><img src="https://github.com/Penryn/picture/blob/main/101.png?raw=true" alt="图片"><br><img src="https://github.com/Penryn/picture/blob/main/102.png?raw=true" alt="图片"><br><img src="https://github.com/Penryn/picture/blob/main/103.png?raw=true" alt="图片"><br><img src="https://github.com/Penryn/picture/blob/main/104.png?raw=true" alt="图片"></li></ul><p>总而言之，这一次还是组队寒软还是很愉快的！！！</p>]]></content>
      
      
      <categories>
          
          <category> 完整项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归</title>
      <link href="/2024/01/28/linear-regression/"/>
      <url>/2024/01/28/linear-regression/</url>
      
        <content type="html"><![CDATA[<p>在学习神经网络之前，我们先了解一些简单的基础知识，我们以线性回归（预测）和softmax回归（分类）为例，了解简单的神经网络架构，数据处理，制定损失函数和如何训练模型。</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>为了解释线性回归，我们举一个实际的例子：我们希望根据房屋的面积（平方米）和房龄（年）来估算房屋价格（元）。为了开发一个能预测房屋价格的模型，我们需要收集一个真实的数据集。这个数据集包括房屋价格、面积和房龄。在机器学习的术语中，该数据集称为训练数据集（training dataset）或训练集（training set）。每行数据（比如一次房屋交易相对应的数据）称为数据样本（sample），也可以称为数据点（data point）或数据实例（data instance）。我们把试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。</p><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><p>假设自变量x和因变量y之间的关系是线性的，即y可以表示x中的元素的加权和，这里通常允许包含一些噪声，在上图体现就是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下式：<br><img src="https://github.com/Penryn/picture/blob/main/10.jpg?raw=true" alt="图片"></p><p>W称为权重（weight），权重决定了每个特征对我们预测值的影响。b称为偏置（bias）、偏移量（offset）或截距（intercept）。偏置是指当所有特征都取值为0时，预测值应该为多少。即使现实中不会有任何房屋的面积是0或房龄正好是0年，我们仍然需要偏置项。如果没有偏置项，我们的模型的表达能力将受到限制。严格来说，式（3.1）是输入特征的一个仿射变换（affine transformation）。仿射变换的特点是通过加权和对特征进行线性变换（linear transformation），并通过偏置项进行平移（translation）。</p><p>给定一个数据集，我们的目标是寻找模型的权重w和偏置b，使得根据模型做出的预测大体符合数据中的真实价格。输出的预测值由输入特征通过线性模型的仿射变换确定，仿射变换由所选权重和偏置确定。</p><p>而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。这个过程中的求和将使用广播机制。给定训练数据特征X和对应的已知标签y，线性回归的目标是找到一组权重向量w和偏置b：当给定从X的同分布中抽样的新样本特征时，这组权重向量和偏置能够使新样本预测标签的误差尽可能小。</p><p>虽然我们确信给定x预测y的最佳模型是线性的，但我们很难找到一个理想的数据集。所以无论我们使用什么方式来观测征X和标签y，都可能会出现少量的观测误差。因此，即使确信特征与标签的潜在关系是呈线性的，我们也会加入一个噪声项以考虑观测误差带来的影响。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>我们要考虑模型拟合程度的度量，这时候就要考虑损失函数（loss function），他可以量化目标的实际值与预测值之间的差距。通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。回归问题中最常用的损失函数是平方误差函数。<br><img src="https://github.com/Penryn/picture/blob/main/11.jpg?raw=true" alt="图片"><br>常数不会带来本质区别，但是形式上会更简单一些，对损失函数求导后常数系数为1</p><p>由于平方误差函数中的二次方项，估计值j(i)和观测值y(2)之较大的差距将导致更大的损失。为了度量模型在整个数据集上测质量，我们需计算在训练集n个样本上的损失均值（等价求和）<br><img src="https://github.com/Penryn/picture/blob/main/15.jpg?raw=true" alt="图片"></p><p>在训练模型时，我们希望寻找一组参数，这组参数能最小化在所有训练样本上的失，如下式<br><img src="https://github.com/Penryn/picture/blob/main/16.jpg?raw=true" alt="图片"></p><h3 id="更新模型"><a href="#更新模型" class="headerlink" title="更新模型"></a>更新模型</h3><p>为了寻找最佳的W和b，我们除了需要模型质量的度量方式，还要一种能够更新模型以提高模型预测质量的方法</p><h4 id="解析解"><a href="#解析解" class="headerlink" title="解析解"></a>解析解</h4><p>线性回归恰好是一个很简单的优化问题，它的解可以用一个式子简单表示，这类解叫做解析解。我们先将偏置b合并到参数w中，合并方法是在包含所有参数的矩阵中附加一列。我们的预测问题是最小化|y-Xw|的平方。这在损失平面上只有一个临界点，这个临界点对应于整个区域的损失极小值点。将损失关于w的导数设为0，得到解析解：<br><img src="https://github.com/Penryn/picture/blob/main/12.jpg?raw=true" alt="图片"></p><p>像线性回归这样的简单问题存在解析解，但并不是所有问题都存在解析解。解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习中。</p><h4 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h4><p>即使在无法得到解析解的情况下，我们也可以有效地训练模型。在许多任务中，那些难以优化的模型效果会更好。</p><p>梯度下降的最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。但实际中的执行可能会非常慢，因为在每次更新参数之前，我们必须遍历整个数据集。因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫作小批量随机梯度下降（minibatch stochastic gradient descent）。</p><p>在每次迭代中，我们先随机抽取一个小批量B，它是由固定数量的训练样本组成的；然后，计算小批量的损失均值关于模型参数的导数（也可以称为梯度）；最后，将梯度乘以一个预先确定的正数η，并从当前参数的值中减掉。<br>我们用下面的数学公式来表示这一更新过程：<br><img src="https://github.com/Penryn/picture/blob/main/13.jpg?raw=true" alt="图片"></p><p>简单来说，该算法第一步是初始化模型的参数的值，如随机初始化，第二步是从数据集中随机抽取小批量样本且在负梯度方向更新参数，并不断迭代这一过程。对于平方损失和仿射变换，我们可以明确写成如下形式：<br><img src="https://github.com/Penryn/picture/blob/main/14.jpg?raw=true" alt="图片"></p><p>｜B｜表示每个小批量中的样本数，也称为批量大小（batch size）。η表示学习率（learning rate）。批量大小和学习率的值通常是预先手动指定，而不是通过模型训练得到的。这些可以调整但不在训练过程中更新的参数称为超参数（hyperparameter）。调参（hyperparametertuning）是选择超参数的过程。超参数通常是我们根据训练迭代结果来调整的，而训练迭代结果是在独立的验证数据集（validation dataset）上评估得到的。</p><p>在训练了预先确定的若干迭代次后（或者直到满足某些其他停止条件后），我们记录下模型参数的估计值，表示为w，b。但是，即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值，因为算法会使损失向最小值缓慢收敛，但不能在有限的步数内非常精确地达到最小值。</p><p>线性回归恰好是一个在整个域中只有一个最小值的学习问题。但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。深度学习实践者很少会花费大力气寻找这样一组参数，使在训练集上的损失达到最小值。事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较小的损失，这一挑战称为泛化（generalization）。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>在基本了解线性回归模型后，我们可以尝试用代码的形式体现。</p><h4 id="从零开始实现"><a href="#从零开始实现" class="headerlink" title="从零开始实现"></a>从零开始实现</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># 定义一个函数，它每次返回batch_size（批量大小）个随机样本的特征和标签</span><br><span class="line">def synthetic_data(w, b, num_examples):  #@save</span><br><span class="line">    """Generate y = Xw + b + noise."""</span><br><span class="line">    X = torch.normal(0, 1, (num_examples, len(w)))</span><br><span class="line"># 生成一个张量X，它的值来自均值为0，标准差为1的正态分布。张量的形状由num_examples和len(w)决定</span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    y += torch.normal(0, 0.01, y.shape)</span><br><span class="line"># 给y添加一些噪声，噪声的值来自均值为0，标准差为0.01的正态分布</span><br><span class="line">    return X, y.reshape((-1, 1))</span><br><span class="line"># y被重塑为列向量。-1表示通过数据的形状和其他维度的值推断出该值</span><br><span class="line"></span><br><span class="line">def data_iter(batch_size, features, labels):  #@save</span><br><span class="line">    """Iterate through a dataset."""</span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line"># 将样本的索引存储在列表indices中</span><br><span class="line">    random.shuffle(indices)</span><br><span class="line"># 样本的读取顺序是随机的</span><br><span class="line">    for i in range(0, num_examples, batch_size):</span><br><span class="line"># 从0开始，每次以batch_size为步长递增，直到len(indices)</span><br><span class="line">        batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)])</span><br><span class="line"># 最后一次可能不足一个批量</span><br><span class="line">    yield features[batch_indices], labels[batch_indices]</span><br><span class="line"></span><br><span class="line"># 定义了线性回归的矢量计算表达式</span><br><span class="line">def linreg(X, w, b):  #@save</span><br><span class="line">    """The linear regression model."""</span><br><span class="line">    return torch.matmul(X, w) + b</span><br><span class="line"></span><br><span class="line"># 定义了损失函数</span><br><span class="line"># 通过广播机制，y的形状转换为y_hat的形状</span><br><span class="line"># 除以2是为了抵消平方的导数，使得计算的梯度更简洁</span><br><span class="line">def squared_loss(y_hat, y):  #@save</span><br><span class="line">    """Squared loss."""</span><br><span class="line">    return (y_hat - y.reshape(y_hat.shape))**2 / 2</span><br><span class="line"></span><br><span class="line"># 定义了优化算法</span><br><span class="line">def sgd(params, lr, batch_size):  #@save</span><br><span class="line">    """Minibatch stochastic gradient descent."""</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for param in params:</span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([2, -3.4])</span><br><span class="line">true_b = 4.2</span><br><span class="line"># 定义了真实的权重true_w和偏差true_b。我们将使用这些参数来生成我们的数据集</span><br><span class="line">features, labels = synthetic_data(true_w, true_b, 1000)</span><br><span class="line"># 调用synthetic_data函数生成1000个数据点的特征和标签</span><br><span class="line">print('features:', features[0],'\nlabel:', labels[0])</span><br><span class="line"></span><br><span class="line">batch_size = 10</span><br><span class="line"># 设置批次大小为10</span><br><span class="line">w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)</span><br><span class="line">b = torch.zeros(1, requires_grad=True)</span><br><span class="line"># 初始化权重w和偏差b。w的值来自均值为0，标准差为0.01的正态分布，b的值为0。</span><br><span class="line"># requires_grad=True表示w和b需要计算梯度，这对于后续的优化步骤是必要的</span><br><span class="line"># 通过requires_grad=True来告知系统需要记录与它们相关的计算，这样系统在反向传播过程中就会记录下与这些变量相关的梯度</span><br><span class="line"></span><br><span class="line"># 定义了小批量随机梯度下降优化器,它通过不断迭代模型参数来优化损失函数</span><br><span class="line"># 学习率</span><br><span class="line">lr = 0.03</span><br><span class="line"># 迭代周期数，即训练的次数</span><br><span class="line">num_epochs = 10</span><br><span class="line"># 网络模型</span><br><span class="line">net = linreg</span><br><span class="line"># 损失函数</span><br><span class="line">loss = squared_loss</span><br><span class="line">for epoch in range(num_epochs):</span><br><span class="line">    # 外层循环是对训练周期的迭代。每个训练周期(epoch)都会将整个数据集在神经网络中前向和后向传递一次</span><br><span class="line">    # 假设样本的数量可以除以批处理的大小，所有训练数据集中的示例在一个epoch中使用一次迭代。小批量例子的特征和标签分别用X和y表示</span><br><span class="line">    for X, y in data_iter(batch_size, features, labels):</span><br><span class="line">        # 内层循环是对数据批次的迭代。data_iter函数从特征和标签中生成大小为batch_size的批次。对于每个批次，X是特征，y是标签</span><br><span class="line">        l = loss(net(X, w, b), y)</span><br><span class="line">        # 通过首先将特征X通过当前权重w和偏差b的模型net，然后使用损失函数loss将输出与真实标签y进行比较，来计算当前批次的损失</span><br><span class="line">        l.sum().backward()</span><br><span class="line">        # 通过在损失张量上调用backward()来计算损失相对于模型参数的梯度。</span><br><span class="line">        # 在调用backward()之前使用sum()函数对批次的损失进行求和，因为PyTorch期望backward()的是标量值张量。</span><br><span class="line">        sgd([w, b], lr, batch_size)</span><br><span class="line">        # 使用计算出的梯度和随机梯度下降（SGD）优化器sgd更新模型参数。学习率lr和批次大小batch_size作为参数传递给优化器。</span><br><span class="line"># 由于我们之前设批量大小batch_size为10，每个小批量的损失l的形状为(10,)，而不是一个标量。</span><br><span class="line"># 因此，我们通过调用l.sum()将其归约为标量，从而调用backward得到标量的梯度。</span><br><span class="line"># 优化器实例sgd是一个函数，它将需要更新的参数作为输入，并在函数内部更新它们的值。</span><br><span class="line"># 由于参数w和b通过net函数被传递给优化器，所以优化器知道需要更新哪些参数</span><br><span class="line"># 在每个训练周期结束时，我们通过net(features)生成模型net的输出，并与标签labels进行比较。</span><br><span class="line"># 为了计算整个数据集上的模型的误差，我们计算所有预测值和真实标签之间的总损失，即l.sum()</span><br><span class="line"># 在每个训练周期结束时，我们通过net(features)生成模型net的输出，并与标签labels进行比较。</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')</span><br><span class="line">    # 在每个训练周期结束后，计算并打印整个数据集的平均损失。使用torch.no_grad()</span><br><span class="line">    # 上下文管理器防止这些操作在计算图中被跟踪，因为我们不需要为这些操作计算梯度</span><br><span class="line">    # 在打印语句中的:f是浮点数的格式规范，它通过将损失格式化为十进制数，使损失更易于阅读</span><br><span class="line">    # train_l.mean()计算当前训练周期中整个数据集的平均损失。float()函数用于将损失（一个PyTorch张量）转换为Python浮点数。</span><br><span class="line">    # 由于我们用l.sum()对损失求和，所以train_l是一个形状为(1,)的张量。嵌套的mean()函数返回一个标量，即一个形状为()的张量。</span><br><span class="line">    # 由于train_l.mean()是一个标量，我们可以直接打印它而不是打印它的值</span><br><span class="line">print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}')</span><br><span class="line">print(f'error in estimating b: {true_b - b}')</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h4 id="借助框架实现"><a href="#借助框架实现" class="headerlink" title="借助框架实现"></a>借助框架实现</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">from torch.utils import data</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([2, -3.4])</span><br><span class="line">true_b = 4.2</span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, 1000)</span><br><span class="line"></span><br><span class="line">def load_array(data_arrays, batch_size, is_train=True):</span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    return data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line">    # 通过TensorDataset和DataLoader类来实现数据的读取功能</span><br><span class="line">    # shuffle参数表示是否打乱数据集中的样本顺序</span><br><span class="line">batch_size = 10</span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br><span class="line">next(iter(data_iter))</span><br><span class="line"># # 通过data_iter的next()函数来读取第一个小批量数据样本</span><br><span class="line"># # next()函数返回的每个样本都是一个形状为(10, 2)的小批量，其中的10是批量大小，2是每个样本的特征数</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(2, 1))</span><br><span class="line"># nn.Sequential类为串联在一起的多个层定义了一个容器</span><br><span class="line"># 当给定输入数据，nn.Sequential实例将数据传递给第一层，然后将第一层的输出作为第二层的输入，以此类推</span><br><span class="line"># 在我们的线性模型示例中，我们的模型只包含一个nn.Linear实例，我们将在后面的章节中继续介绍更复杂的例子</span><br><span class="line"># 我们将nn.Linear实例称为图层，它是一个包含权重和偏置的神经网络组件，它将输入映射到输出</span><br><span class="line"># 在PyTorch中，全连接层在其weight属性中存储权重，在其bias属性中存储偏置</span><br><span class="line"># 由于我们只想要对权重和偏置进行梯度下降，因此我们将其设置为requires_grad=True</span><br><span class="line">net[0].weight.data.normal_(0, 0.01)# w</span><br><span class="line">net[0].bias.data.fill_(0)# b</span><br><span class="line"># 初始化模型的权重和偏差。</span><br><span class="line"># 权重的值来自均值为0，标准差为0.01的正态分布，偏差的值为0。</span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=0.03)</span><br><span class="line">num_epochs = 3</span><br><span class="line">for epoch in range(num_epochs):</span><br><span class="line">    for X, y in data_iter:</span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    # 在每个训练周期结束后，通过net(features)生成模型net的输出，并与标签labels进行比较，来计算整个数据集的损失。</span><br><span class="line">    print(f'epoch {epoch + 1}, loss {l:f}')</span><br><span class="line">w = net[0].weight.data</span><br><span class="line">print('error in estimating w', true_w - w.reshape(true_w.shape))</span><br><span class="line">b = net[0].bias.data</span><br><span class="line">print('error in estimating b', true_b - b)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>相信看到这里你应该对线性回归有简单的认识了。</p>]]></content>
      
      
      <categories>
          
          <category> ai </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩阵与线性变换</title>
      <link href="/2024/01/24/linear-transformation/"/>
      <url>/2024/01/24/linear-transformation/</url>
      
        <content type="html"><![CDATA[<p>明明线代已经考完了，笔者现在才重新在几何理解线代，属实有点奇怪。<br>今天在b站的<a href="https://space.bilibili.com/88461692">3Blue1Brown</a>听他的线性代数本质，清晰了许多。</p><p><strong>很遗憾，矩阵是什么是说不清的，你必须得自己看看。——墨菲斯</strong></p><h3 id="矩阵与向量相乘"><a href="#矩阵与向量相乘" class="headerlink" title="矩阵与向量相乘"></a>矩阵与向量相乘</h3><p>首先，我们可以知道，直角坐标系中，任意一个向量可以用两个最基本的正交基向量表示，而将两个向量变换（不改变原点位置，直线仍然是直线，对应平行线依旧平行）后的位置，他们坐标会发生变化，相应的，他们所构成的向量也会发生变化。</p><p>事实上，我们想要知道变换后的向量的坐标，我们只需要变换后的基向量的坐标和他们之间的几何关系就可以求得。</p><p>而变化后的基向量的坐标（3，-2）（2，1）组成一个矩阵，而原线性关系为（5，7），具体可看下面的图片<br><img src="https://github.com/Penryn/picture/blob/main/1.png?raw=true" alt="图片"><br><img src="https://github.com/Penryn/picture/blob/main/3.png?raw=true" alt="图片"></p><p>将其抽象出来，我们可以得到矩阵乘于向量的几何含义，就是通过这样的运算得到所构成向量的新坐标。<br><img src="https://github.com/Penryn/picture/blob/main/4.png?raw=true" alt="图片"></p><h3 id="矩阵矩阵相乘"><a href="#矩阵矩阵相乘" class="headerlink" title="矩阵矩阵相乘"></a>矩阵矩阵相乘</h3><p>在了解过上面矩阵与向量相乘的几何意义后，我们可以进一步理解矩阵与矩阵相乘的几何意义<br>在矩阵与矩阵相乘时，我们是要从右边的矩阵看到左边的矩阵，右边的矩阵可以看作若干个向量所构成的矩阵，然后这个向量以左边矩阵的变化。即相当于原来最初的基向量先变成右边矩阵的形式，在分别每列进行左边矩阵的变换。<br><img src="https://github.com/Penryn/picture/blob/main/5.png?raw=true" alt="图片"><br><img src="https://github.com/Penryn/picture/blob/main/6.png?raw=true" alt="图片">  </p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gorm的多种关联方式</title>
      <link href="/2023/11/23/gorm-association/"/>
      <url>/2023/11/23/gorm-association/</url>
      
        <content type="html"><![CDATA[<p><a href="https://gorm.io/zh_CN/docs/belongs_to.html">Gorm官网</a>上将这个分成了belongs to，has one，has many，many to many这些关系，今天笔者就简单分成这三种关系（一对一，一对多和多对多）来逐一讲解。</p><hr><h2 id="一对一"><a href="#一对一" class="headerlink" title="一对一"></a>一对一</h2><p>以笔者写的一份项目为例（一个用户对应一个用户信息）</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">package models</span><br><span class="line"></span><br><span class="line">type User struct {</span><br><span class="line">UserID     int      `json:"-" gorm:"primaryKey"`</span><br><span class="line">Name       string   `json:"name"`</span><br><span class="line">Password   []byte   `json:"-"`</span><br><span class="line">Userinfo   Userinfo `json:"-"`</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">type Userinfo struct {</span><br><span class="line">ID       int    `json:"-" `</span><br><span class="line">UserID   int    `json:"-"`</span><br><span class="line">Name     string `json:"name"`</span><br><span class="line">Phone    string `json:"phone"`</span><br><span class="line">Email    string `json:"email"`</span><br><span class="line">Birthday string `json:"birthday"`</span><br><span class="line">Address  string `json:"address"`</span><br><span class="line">Motto    string `json:"motto"`</span><br><span class="line">Avatar   string `json:"avatar"`</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><ol><li>增</li></ol><ul><li><p>创建用户并携带相关个人信息</p></li><li><p>新建用户并关联已有信息</p></li><li><p>新建个人信息并关联已有用户</p></li><li><p>已有用户关联已有信息</p></li></ul><ol start="2"><li>删</li></ol><ul><li><p>清除用户与信息的关系</p></li><li><p>删除用户并连带信息一起删除</p></li><li><p>仅删除信息保留用户</p></li><li><p>仅删除用户保留信息</p></li></ul><ol start="3"><li>改</li></ol><ul><li><p>直接通过信息表修改内容</p></li><li><p>通过用户表修改内容</p></li></ul><ol start="4"><li>查</li></ol><p>（等笔者找个时间补上）</p>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>手把手教你搭建属于你自己的hexo博客，并布置在github page上</title>
      <link href="/2023/11/11/createblog/"/>
      <url>/2023/11/11/createblog/</url>
      
        <content type="html"><![CDATA[<p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p><hr><h2 id="1-环境配置"><a href="#1-环境配置" class="headerlink" title="1.环境配置"></a>1.环境配置</h2><ul><li>Node.js</li><li>Git</li><li>修改npm为淘宝镜像源，并设置cnpm</li></ul><ol><li><p>环境的下载只需点开网站找到适合自己的版本安装即可，这里不赘述。</p></li><li><p>安装完毕后可以通过cmd命令行输入node -v,npm -v和git –version来验证，如果出现下图则安装成功。<br>  <img src="/../picture/createblog/1.png" alt="图片"></p></li><li><p>修改npm的镜像源为在国内更为稳定的淘宝镜像源（建议永久设置）</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">临时改变镜像源 </span><br><span class="line">npm --registry=https://registry.npm.taobao.org</span><br><span class="line"></span><br><span class="line">永久设置为淘宝镜像源</span><br><span class="line">npm config set registry https://registry.npm.taobao.org</span><br><span class="line"></span><br><span class="line">cnpm安装，在国外服务器不佳时就可以用cnpm代替npm命令</span><br><span class="line">npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br></pre></td></tr></tbody></table></figure></li></ol><hr><h2 id="2-github准备"><a href="#2-github准备" class="headerlink" title="2.github准备"></a>2.github准备</h2><ol><li><p>打开<a href="https://github.com/">github</a>,并登录或注册你的账号</p></li><li><p>新建一个格式为你的用户名.github.io的仓库，并设置为公开<br> <img src="/../picture/createblog/2.png" alt="图片"><br> <img src="/../picture/createblog/3.png" alt="图片"></p></li><li><p>创建成功后在桌面点击右键，Git Bash Here，打开Git的命令行输入这两行代码</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name "此处填写你注册github时的用户名"</span><br><span class="line">git config --global user.email "此处填写你注册github时的邮箱"</span><br></pre></td></tr></tbody></table></figure></li><li><p>然后就可以在C:/Users/[电脑登录的用户名]/下找到.gitconfig文件（如果没能找到，请打开显示windows显示隐藏文件的功能），用编辑器打开看到以下内容代表配置成功。<br><img src="/../picture/createblog/5.png" alt="图片"></p></li></ol><hr><h2 id="3-安装Hexo"><a href="#3-安装Hexo" class="headerlink" title="3.安装Hexo"></a>3.安装Hexo</h2><ol><li>首先新建一个文件夹👀作为你的博客文件的存放位置，点进去打开Git命令行分别输入<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># hexo框架的安装</span><br><span class="line">npm install -g hexo-cli</span><br><span class="line">&nbsp; </span><br><span class="line"># 等上一个命令完成后，再输入下面的命令</span><br><span class="line">hexo init</span><br><span class="line">&nbsp; &nbsp; </span><br><span class="line"># 安装博客所需要的依赖文件(如果上面安装了cnpm则可以把下面的npm换成cnpm)</span><br><span class="line">npm install</span><br></pre></td></tr></tbody></table></figure></li><li>等待运行完成，你会发现此时文件夹内多了好多文件。此时本地搭建完成，我们来运行一下试试看，输入以下命令.<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g           </span><br><span class="line">hexo s                                          </span><br></pre></td></tr></tbody></table></figure><img src="/../picture/createblog/4.png" alt="图片"></li><li>根据提示我们打开 <a href="http://localhost:4000/">http://localhost:4000</a> ，就可以看到生成的网页，说明Hexo已经成功在本地运行.</li></ol><hr><h2 id="4-发布到github"><a href="#4-发布到github" class="headerlink" title="4.发布到github"></a>4.发布到github</h2><p>我们已经完成了Hexo下载安装和本地运行，接下来将本地博客发布到Github让别人也能通过网址访问你的博客。</p><ol><li><p>在博客所在文件夹下打开Git命令行，分别输入以下命令(如果上面安装了cnpm则可以把下面的npm换成cnpm)</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 安装用来发布的插件</span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line">&nbsp;​</span><br><span class="line"># 将本地目录与Github关联起来</span><br><span class="line"># 这步输入后一直回车即可</span><br><span class="line">ssh-keygen -t rsa -C "你的邮箱地址"</span><br></pre></td></tr></tbody></table></figure></li><li><p>在 C:/Users/[电脑登录的用户名] 目录下找到名为.ssh 的文件夹，打开其中的 id_rsa.pub，复制里面的的内容。 然后打开 Github，点击右上角的头像 Settings 选择 SSH and GPG keys。<br><img src="/../picture/createblog/6.png" alt="图片"><br><img src="/../picture/createblog/7.png" alt="图片"></p></li><li><p>点击 New SSH key 将之前复制的内容粘帖到 Key 的框中，Title 可以随意，点击 Add SSH key 完成添加<br><img src="/../picture/createblog/8.png" alt="图片"></p></li><li><p>回到命令行界面测试是否与Github连接成功，输入ssh -T <a href="mailto:git@github.com">git@github.com</a>，出现一个询问内容输入yes，出现You’ve successfully …说明连接成功。</p></li><li><p>进入博客站点目录，用文本编辑器打开_config.yml，这个_config.yml 是博客的配置文件，在以后会经常使用到，修改如下图的几个地方：<br><img src="/../picture/createblog/9.png" alt="图片"></p></li><li><p>然后滑到文件最底部deploy处添加如下代码：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&nbsp; type: git</span><br><span class="line">&nbsp; repo: git@github.com:github用户名/github用户名.github.io.git &nbsp;        </span><br><span class="line">&nbsp; branch: master     (或者是main)</span><br></pre></td></tr></tbody></table></figure></li></ol><p>7.最后一步，生成页面并发布，我们执行如下命令</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&nbsp; # generate, Hexo会根据配置文件渲染出一套静态页面</span><br><span class="line">&nbsp; hexo g</span><br><span class="line">&nbsp;​</span><br><span class="line">&nbsp; # deploy, 将上一步渲染出的一系列文件上传至至Github Pages</span><br><span class="line">&nbsp; hexo d</span><br><span class="line">&nbsp;​</span><br><span class="line">&nbsp; # 或者也可以直接输入此命令，直接完成渲染和上传</span><br><span class="line">&nbsp; hexo g -d</span><br></pre></td></tr></tbody></table></figure><p>上传完成后，在浏览器中打开网址 你的github用户名.github.io，查看上传的网页。如果页面变成了之前本地调试时的样子，说明上传完成了。没变的话查看一下上传时命令行窗口的信息有没有错误信息，没有的话等一下或按ctrl+f5刷新清除一下浏览器缓存试试。</p>]]></content>
      
      
      <categories>
          
          <category> 博客搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
